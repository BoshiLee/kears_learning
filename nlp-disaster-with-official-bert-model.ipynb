{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q tf-models-official==2.7.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom official.nlp import optimization  # to create AdamW optimizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"cell_type":"code","source":"current_directory = os.getcwd()\nprint(current_directory)\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.columns)\ntrain.head(5)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.sample(5)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_label = train.target.to_numpy()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train_label.reshape(-1)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering","metadata":{}},{"cell_type":"code","source":"train.drop(['id', 'keyword', 'location', 'target'], axis=1, inplace=True)\ntest.drop(['id', 'keyword', 'location'], axis=1, inplace=True)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.to_numpy()\ntest = test.to_numpy()\nX_train = train.reshape(-1)\nX_test = test.reshape(-1)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train[0])","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Normalization","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"import re\nimport string\n\ndef clean_tweet(tweet):\n\n  #apostrophe\n  tweet = re.sub(r'\\x89Ûª', \"'\", tweet)\n  tweet = re.sub(r'\\x89Û÷', \"'\", tweet)\n\n  #quotation\n  tweet = re.sub(r'\\x89ÛÏ', '\"', tweet)\n  tweet = re.sub(r'\\x89Û\\x9d', '\"', tweet)\n\n  #hyphen\n  tweet = re.sub(r'\\x89ÛÒ', '-', tweet)\n  tweet = re.sub(r'\\x89ÛÓ', '—', tweet)\n\n  #euro\n  tweet = re.sub(r'\\x89âÂ', '€', tweet)\n\n  #ellipsis\n  tweet = re.sub(r'\\x89Û_', '...', tweet)\n\n  #amp\n  tweet = re.sub(r'&', 'and', tweet)\n\n\n  #bullet\n  tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n  tweet = re.sub(r'\\x89Û¢', '', tweet)\n\n\n  #no idea\n  tweet = re.sub(r'\\x89ã¢', '', tweet)\n  tweet = re.sub(r\"å_\", \"\", tweet)\n\n\n  #other\n  tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n  tweet = re.sub(r\"åÊ\", \"\", tweet)\n  tweet = re.sub(r\"åÈ\", \"\", tweet)\n  tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)\n  tweet = re.sub(r\"Ì©\", \"e\", tweet)\n  tweet = re.sub(r\"å¨\", \"\", tweet)\n  tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n  tweet = re.sub(r\"åÇ\", \"\", tweet)\n  tweet = re.sub(r\"å£3million\", \"3 million \", tweet)\n  tweet = re.sub(r\"åÀ\", \"\", tweet)\n\n  return tweet\n\ndef find_URL(text):\n  url = re.compile(r'https?://\\S+|www\\.\\S+')\n\n  return url.findall(text)\n\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n  html=re.compile(r'<.*?>')\n  return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_standardization(input_data):\n    r = clean_tweet(input_data)\n    r = remove_URL(r)\n    r = remove_html(r)\n    r = remove_emoji(r)\n    r = str.lower(r)\n    return r","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef normalization(tweets):\n    result = []\n    for tweet in tweets:\n        result.append(custom_standardization(tweet))\n    return result\n\nX_train = normalization(X_train)\nX_test = normalization(X_test)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model constants.\nmax_features = 20000\nembedding_dim = 128\nmax_len = 500","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading models from TensorFlow Hub","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'\n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_base/2',\n    'electra_small':\n        'https://tfhub.dev/google/electra_small/2',\n    'electra_base':\n        'https://tfhub.dev/google/electra_base/2',\n    'experts_pubmed':\n        'https://tfhub.dev/google/experts/bert/pubmed/2',\n    'experts_wiki_books':\n        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'electra_small':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'electra_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_pubmed':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_wiki_books':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test preprocessing model","metadata":{}},{"cell_type":"code","source":"text_test = [X_test[0]]\ntext_preprocessed = bert_preprocess_model(text_test)\n\nprint(f'Keys       : {list(text_preprocessed.keys())}')\nprint(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\nprint(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\nprint(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\nprint(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"text preprocessor is a TensorFlow model, It can be included in your model directly.","metadata":{}},{"cell_type":"code","source":"bert_model = hub.KerasLayer(tfhub_handle_encoder)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_results = bert_model(text_preprocessed)\n\nprint(f'Loaded BERT: {tfhub_handle_encoder}')\nprint(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\nprint(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\nprint(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\nprint(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Dataset","metadata":{}},{"cell_type":"code","source":"## Spilt Data to train and validation\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=1)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\nbatch_size = 32\nseed = 42\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ntrain_ds = train_ds.batch(batch_size=batch_size).cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\nval_ds = val_ds.batch(batch_size=batch_size).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = tf.data.Dataset.from_tensor_slices(X_test)\ntest_ds = test_ds.batch(batch_size=batch_size).cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check Data\nIs important to test your dataset everytime.","metadata":{}},{"cell_type":"code","source":"for text_batch, label_batch in train_ds.take(1):\n  for i in range(3):\n    print(f'Review: {text_batch.numpy()[i]}')\n    label = label_batch.numpy()[i]\n    print(f'Label : {label}')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Bert Model\nuse bert layer and one dense layer as out put","metadata":{}},{"cell_type":"code","source":"def build_classifier_model():\n  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n  encoder_inputs = preprocessing_layer(text_input)\n  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n  outputs = encoder(encoder_inputs)\n  net = outputs['pooled_output']\n  net = tf.keras.layers.Dropout(0.1)(net)\n  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n  return tf.keras.Model(text_input, net)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclassifier_model = build_classifier_model()\nbert_raw_result = classifier_model(tf.constant(text_test))\nprint(tf.sigmoid(bert_raw_result))","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_model.summary()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(classifier_model)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nmetrics = tf.metrics.BinaryAccuracy()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 10\nsteps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\nprint(steps_per_epoch, num_train_steps, num_warmup_steps)\n\ninit_lr = 3e-5\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_model.compile(optimizer=optimizer,\n                         loss=loss,\n                         metrics=metrics)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import *\n\nlr = ReduceLROnPlateau(monitor=\"val_binary_accuracy\", mode='max', factor=0.7, patience=4, verbose=False)\nes = EarlyStopping(monitor='val_binary_accuracy',mode='max', patience=10, verbose=False,restore_best_weights=True)\n\nhistory = classifier_model.fit(\n    x=train_ds,\n    validation_data=val_ds,\n    epochs=epochs,\n\n    )","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = classifier_model.evaluate(train_ds)\nprint(f'Loss: {loss}')\nprint(f'Accuracy: {accuracy}')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot result","metadata":{}},{"cell_type":"code","source":"def plot_hist(history_dict):\n    history_dict = history.history\n    print(history_dict.keys())\n\n    acc = history_dict['binary_accuracy']\n    val_acc = history_dict['val_binary_accuracy']\n    loss = history_dict['loss']\n    val_loss = history_dict['val_loss']\n\n    epochs = range(1, len(acc) + 1)\n    fig = plt.figure(figsize=(10, 6))\n    fig.tight_layout()\n\n    plt.subplot(2, 1, 1)\n    # r is for \"solid red line\"\n    plt.plot(epochs, loss, 'r', label='Training loss')\n    # b is for \"solid blue line\"\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    # plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.subplot(2, 1, 2)\n    plt.plot(epochs, acc, 'r', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    return","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_hist(history_dict=history)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_name = 'nlp_disaster_tweets'\nsaved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n\nclassifier_model.save(saved_model_path, include_optimizer=False)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reloaded_model = tf.saved_model.load(saved_model_path)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"examples = np.array(X_test)\n\nexamples = examples[np.random.choice(100, 5)]\nexamples","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_my_examples(inputs, results):\n  result_for_printing = \\\n    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n                         for i in range(len(inputs))]\n  print(*result_for_printing, sep='\\n')\n  print()\n\nreloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\noriginal_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n\nprint('Results from the saved model:')\nprint_my_examples(examples, reloaded_results)\nprint('Results from the model in memory:')\nprint_my_examples(examples, original_results)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict test data","metadata":{}},{"cell_type":"code","source":"y_pred = classifier_model.predict(X_test).squeeze()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit data","metadata":{}},{"cell_type":"code","source":"sub_data = pd.read_csv(\"data/sample_submission.csv\")","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_data.target = (y_pred > 0.5).astype(int)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_data","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_data.to_csv('submission.csv', index=False)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]}]}