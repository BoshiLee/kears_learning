{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004fb920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdad0901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GitHub\\keras_learning\\training_project\\tabular_playground_series_Apr_2022\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "train = pd.read_csv('data/train.csv')\n",
    "train_labels = pd.read_csv('data/train_labels.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c4d65",
   "metadata": {},
   "source": [
    "In this competition, you'll classify 60-second sequences of sensor data, indicating whether a subject was in either of two activity states for the duration of the sequence.\n",
    "\n",
    "在本次競賽中，您將對 60 秒的感測器資料序列進行分類，找出受試者在序列持續時間內處於兩種活動狀態的其中一種。\n",
    "\n",
    "## Files and Field Descriptions\n",
    "- train.csv - the training set, comprising ~26,000 60-second recordings of thirteen biological sensors for almost one thousand experimental participants\n",
    "    - sequence - a unique id for each sequence\n",
    "    - subject - a unique id for the subject in the experiment\n",
    "    - step - time step of the recording, in one second intervals\n",
    "    - sensor_00 - sensor_12 - the value for each of the thirteen sensors at that time step\n",
    "- train_labels.csv - the class label for each sequence.\n",
    "    - sequence - the unique id for each sequence.\n",
    "    - state - the state associated to each sequence. This is the target which you are trying to predict.\n",
    "- test.csv - the test set. For each of the ~12,000 sequences, you should predict a value for that sequence's state.\n",
    "- sample_submission.csv - a sample submission file in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d849ef78",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         sequence  subject  step  sensor_00  sensor_01  sensor_02  sensor_03  \\\n0               0       47     0  -0.196291   0.112395   1.000000   0.329204   \n1               0       47     1  -0.447450   0.134454   1.000000  -0.658407   \n2               0       47     2   0.326893  -0.694328   1.000000   0.330088   \n3               0       47     3   0.523184   0.751050   1.000000   0.976991   \n4               0       47     4   0.272025   1.074580   1.000000  -0.136283   \n...           ...      ...   ...        ...        ...        ...        ...   \n1558075     25967      327    55  -0.282844  -1.217437  -1.666153   0.586726   \n1558076     25967      327    56   0.130603   0.349790  -1.666153  -0.324779   \n1558077     25967      327    57  -0.579598   0.429622  -1.666153   0.319469   \n1558078     25967      327    58   1.278980   1.711134  -1.522820   0.802655   \n1558079     25967      327    59  -1.136012  -3.702731  -1.332820  -0.766372   \n\n         sensor_04  sensor_05  sensor_06  sensor_07  sensor_08  sensor_09  \\\n0        -1.004660  -0.131638  -0.127505   0.368702       -0.1  -0.963873   \n1         0.162495   0.340314  -0.209472  -0.867176        0.2  -0.301301   \n2         0.473678   1.280479  -0.094718   0.535878        1.4   1.002168   \n3        -0.563287  -0.720269   0.793260   0.951145       -0.3  -0.995665   \n4         0.398579   0.044877   0.560109  -0.541985       -0.9   1.055636   \n...            ...        ...        ...        ...        ...        ...   \n1558075  -0.930698  -0.451010  -0.651184   0.368702        0.4   0.008671   \n1558076   0.775324  -0.332835   0.099271   0.122137       -0.2   0.644509   \n1558077   0.308861   0.282723  -0.512750   0.012214       -1.6  -0.424133   \n1558078  -0.460541  -0.055348   2.405282   0.043511        1.9   0.283960   \n1558079  -0.430027  -0.091997  -2.512750  -0.022901       -1.1  -0.653902   \n\n         sensor_10  sensor_11  sensor_12  \n0        -0.985069   0.531893   4.751492  \n1         0.082733  -0.231481   0.454390  \n2         0.449221  -0.586420  -4.736147  \n3        -0.434290   1.344650   0.429241  \n4         0.812631   0.123457  -0.223359  \n...            ...        ...        ...  \n1558075  -0.723536  -0.353909  -0.914749  \n1558076   0.691407  -0.613169  -0.515772  \n1558077   0.716855   1.628601   0.928389  \n1558078  -0.914914   0.364198   0.211424  \n1558079  -0.418516  -1.453704  -1.561381  \n\n[1558080 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sequence</th>\n      <th>subject</th>\n      <th>step</th>\n      <th>sensor_00</th>\n      <th>sensor_01</th>\n      <th>sensor_02</th>\n      <th>sensor_03</th>\n      <th>sensor_04</th>\n      <th>sensor_05</th>\n      <th>sensor_06</th>\n      <th>sensor_07</th>\n      <th>sensor_08</th>\n      <th>sensor_09</th>\n      <th>sensor_10</th>\n      <th>sensor_11</th>\n      <th>sensor_12</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>47</td>\n      <td>0</td>\n      <td>-0.196291</td>\n      <td>0.112395</td>\n      <td>1.000000</td>\n      <td>0.329204</td>\n      <td>-1.004660</td>\n      <td>-0.131638</td>\n      <td>-0.127505</td>\n      <td>0.368702</td>\n      <td>-0.1</td>\n      <td>-0.963873</td>\n      <td>-0.985069</td>\n      <td>0.531893</td>\n      <td>4.751492</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>47</td>\n      <td>1</td>\n      <td>-0.447450</td>\n      <td>0.134454</td>\n      <td>1.000000</td>\n      <td>-0.658407</td>\n      <td>0.162495</td>\n      <td>0.340314</td>\n      <td>-0.209472</td>\n      <td>-0.867176</td>\n      <td>0.2</td>\n      <td>-0.301301</td>\n      <td>0.082733</td>\n      <td>-0.231481</td>\n      <td>0.454390</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>47</td>\n      <td>2</td>\n      <td>0.326893</td>\n      <td>-0.694328</td>\n      <td>1.000000</td>\n      <td>0.330088</td>\n      <td>0.473678</td>\n      <td>1.280479</td>\n      <td>-0.094718</td>\n      <td>0.535878</td>\n      <td>1.4</td>\n      <td>1.002168</td>\n      <td>0.449221</td>\n      <td>-0.586420</td>\n      <td>-4.736147</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>47</td>\n      <td>3</td>\n      <td>0.523184</td>\n      <td>0.751050</td>\n      <td>1.000000</td>\n      <td>0.976991</td>\n      <td>-0.563287</td>\n      <td>-0.720269</td>\n      <td>0.793260</td>\n      <td>0.951145</td>\n      <td>-0.3</td>\n      <td>-0.995665</td>\n      <td>-0.434290</td>\n      <td>1.344650</td>\n      <td>0.429241</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>47</td>\n      <td>4</td>\n      <td>0.272025</td>\n      <td>1.074580</td>\n      <td>1.000000</td>\n      <td>-0.136283</td>\n      <td>0.398579</td>\n      <td>0.044877</td>\n      <td>0.560109</td>\n      <td>-0.541985</td>\n      <td>-0.9</td>\n      <td>1.055636</td>\n      <td>0.812631</td>\n      <td>0.123457</td>\n      <td>-0.223359</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1558075</th>\n      <td>25967</td>\n      <td>327</td>\n      <td>55</td>\n      <td>-0.282844</td>\n      <td>-1.217437</td>\n      <td>-1.666153</td>\n      <td>0.586726</td>\n      <td>-0.930698</td>\n      <td>-0.451010</td>\n      <td>-0.651184</td>\n      <td>0.368702</td>\n      <td>0.4</td>\n      <td>0.008671</td>\n      <td>-0.723536</td>\n      <td>-0.353909</td>\n      <td>-0.914749</td>\n    </tr>\n    <tr>\n      <th>1558076</th>\n      <td>25967</td>\n      <td>327</td>\n      <td>56</td>\n      <td>0.130603</td>\n      <td>0.349790</td>\n      <td>-1.666153</td>\n      <td>-0.324779</td>\n      <td>0.775324</td>\n      <td>-0.332835</td>\n      <td>0.099271</td>\n      <td>0.122137</td>\n      <td>-0.2</td>\n      <td>0.644509</td>\n      <td>0.691407</td>\n      <td>-0.613169</td>\n      <td>-0.515772</td>\n    </tr>\n    <tr>\n      <th>1558077</th>\n      <td>25967</td>\n      <td>327</td>\n      <td>57</td>\n      <td>-0.579598</td>\n      <td>0.429622</td>\n      <td>-1.666153</td>\n      <td>0.319469</td>\n      <td>0.308861</td>\n      <td>0.282723</td>\n      <td>-0.512750</td>\n      <td>0.012214</td>\n      <td>-1.6</td>\n      <td>-0.424133</td>\n      <td>0.716855</td>\n      <td>1.628601</td>\n      <td>0.928389</td>\n    </tr>\n    <tr>\n      <th>1558078</th>\n      <td>25967</td>\n      <td>327</td>\n      <td>58</td>\n      <td>1.278980</td>\n      <td>1.711134</td>\n      <td>-1.522820</td>\n      <td>0.802655</td>\n      <td>-0.460541</td>\n      <td>-0.055348</td>\n      <td>2.405282</td>\n      <td>0.043511</td>\n      <td>1.9</td>\n      <td>0.283960</td>\n      <td>-0.914914</td>\n      <td>0.364198</td>\n      <td>0.211424</td>\n    </tr>\n    <tr>\n      <th>1558079</th>\n      <td>25967</td>\n      <td>327</td>\n      <td>59</td>\n      <td>-1.136012</td>\n      <td>-3.702731</td>\n      <td>-1.332820</td>\n      <td>-0.766372</td>\n      <td>-0.430027</td>\n      <td>-0.091997</td>\n      <td>-2.512750</td>\n      <td>-0.022901</td>\n      <td>-1.1</td>\n      <td>-0.653902</td>\n      <td>-0.418516</td>\n      <td>-1.453704</td>\n      <td>-1.561381</td>\n    </tr>\n  </tbody>\n</table>\n<p>1558080 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cfe9b26",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       sequence  state\n0             0      0\n1             1      1\n2             2      1\n3             3      1\n4             4      1\n...         ...    ...\n25963     25963      1\n25964     25964      0\n25965     25965      1\n25966     25966      1\n25967     25967      0\n\n[25968 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sequence</th>\n      <th>state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25963</th>\n      <td>25963</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25964</th>\n      <td>25964</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25965</th>\n      <td>25965</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25966</th>\n      <td>25966</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25967</th>\n      <td>25967</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>25968 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a150d70",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        sequence  subject  step  sensor_00  sensor_01  sensor_02  sensor_03  \\\n0          25968      684     0   2.427357  19.639706    1.00000  -1.466372   \n1          25968      684     1  -4.950541 -21.747899    1.00000   0.983186   \n2          25968      684     2   1.136012 -10.756303    1.00000   1.016814   \n3          25968      684     3   0.806028   6.504202    1.00000  -0.179646   \n4          25968      684     4   1.288253   5.552521    1.00000  -0.493805   \n...          ...      ...   ...        ...        ...        ...        ...   \n733075     38185      773    55   0.211747   2.005252   -1.33282   0.695575   \n733076     38185      773    56  -0.826121  -2.468487   -1.33282   0.381416   \n733077     38185      773    57   0.755023   1.469538   -1.33282  -1.253097   \n733078     38185      773    58  -0.187017   0.714286   -1.33282   0.077876   \n733079     38185      773    59  -0.414992  -2.858193   -1.33282   1.061062   \n\n        sensor_04  sensor_05  sensor_06  sensor_07  sensor_08  sensor_09  \\\n0       -1.289973  -4.207928   2.486339  -2.493893        8.0  -1.123555   \n1       -0.569053   1.845924  -3.887978   1.727481       -2.9   0.395231   \n2        0.964157   2.454749   0.312386   1.154198       -5.6   1.114162   \n3        0.969221  -1.035153  -0.457195   0.254962       -2.7  -0.588873   \n4       -1.036124  -1.126402   2.008197  -0.730534        0.0   0.899566   \n...           ...        ...        ...        ...        ...        ...   \n733075  -0.161327  -1.193717   0.421676   0.869466        0.0  -1.536850   \n733076   0.144745   1.060583  -0.765938   0.288550        0.2  -1.956647   \n733077  -0.414802   0.007479   0.907104  -1.556489        0.4   4.341763   \n733078   1.323245   0.159312  -0.397996   0.306870        0.1  -1.013728   \n733079  -0.264150  -0.449514  -0.601093   1.621374       -1.0  -3.650289   \n\n        sensor_10  sensor_11  sensor_12  \n0       -1.673048  10.980453   0.419011  \n1       -0.882233  -1.871399  -0.008525  \n2        1.525273 -11.584362   0.139812  \n3        0.608761  -4.241770  -0.462916  \n4       -1.259615  -0.472222  -0.121483  \n...           ...        ...        ...  \n733075   0.388101   2.205761 -91.610827  \n733076  -0.032158  -1.794239  72.414749  \n733077   0.150273   0.641975 -34.065644  \n733078  -0.608616   0.317901  65.659420  \n733079  -0.147107  -1.559671  57.189685  \n\n[733080 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sequence</th>\n      <th>subject</th>\n      <th>step</th>\n      <th>sensor_00</th>\n      <th>sensor_01</th>\n      <th>sensor_02</th>\n      <th>sensor_03</th>\n      <th>sensor_04</th>\n      <th>sensor_05</th>\n      <th>sensor_06</th>\n      <th>sensor_07</th>\n      <th>sensor_08</th>\n      <th>sensor_09</th>\n      <th>sensor_10</th>\n      <th>sensor_11</th>\n      <th>sensor_12</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25968</td>\n      <td>684</td>\n      <td>0</td>\n      <td>2.427357</td>\n      <td>19.639706</td>\n      <td>1.00000</td>\n      <td>-1.466372</td>\n      <td>-1.289973</td>\n      <td>-4.207928</td>\n      <td>2.486339</td>\n      <td>-2.493893</td>\n      <td>8.0</td>\n      <td>-1.123555</td>\n      <td>-1.673048</td>\n      <td>10.980453</td>\n      <td>0.419011</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25968</td>\n      <td>684</td>\n      <td>1</td>\n      <td>-4.950541</td>\n      <td>-21.747899</td>\n      <td>1.00000</td>\n      <td>0.983186</td>\n      <td>-0.569053</td>\n      <td>1.845924</td>\n      <td>-3.887978</td>\n      <td>1.727481</td>\n      <td>-2.9</td>\n      <td>0.395231</td>\n      <td>-0.882233</td>\n      <td>-1.871399</td>\n      <td>-0.008525</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>25968</td>\n      <td>684</td>\n      <td>2</td>\n      <td>1.136012</td>\n      <td>-10.756303</td>\n      <td>1.00000</td>\n      <td>1.016814</td>\n      <td>0.964157</td>\n      <td>2.454749</td>\n      <td>0.312386</td>\n      <td>1.154198</td>\n      <td>-5.6</td>\n      <td>1.114162</td>\n      <td>1.525273</td>\n      <td>-11.584362</td>\n      <td>0.139812</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>25968</td>\n      <td>684</td>\n      <td>3</td>\n      <td>0.806028</td>\n      <td>6.504202</td>\n      <td>1.00000</td>\n      <td>-0.179646</td>\n      <td>0.969221</td>\n      <td>-1.035153</td>\n      <td>-0.457195</td>\n      <td>0.254962</td>\n      <td>-2.7</td>\n      <td>-0.588873</td>\n      <td>0.608761</td>\n      <td>-4.241770</td>\n      <td>-0.462916</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>25968</td>\n      <td>684</td>\n      <td>4</td>\n      <td>1.288253</td>\n      <td>5.552521</td>\n      <td>1.00000</td>\n      <td>-0.493805</td>\n      <td>-1.036124</td>\n      <td>-1.126402</td>\n      <td>2.008197</td>\n      <td>-0.730534</td>\n      <td>0.0</td>\n      <td>0.899566</td>\n      <td>-1.259615</td>\n      <td>-0.472222</td>\n      <td>-0.121483</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>733075</th>\n      <td>38185</td>\n      <td>773</td>\n      <td>55</td>\n      <td>0.211747</td>\n      <td>2.005252</td>\n      <td>-1.33282</td>\n      <td>0.695575</td>\n      <td>-0.161327</td>\n      <td>-1.193717</td>\n      <td>0.421676</td>\n      <td>0.869466</td>\n      <td>0.0</td>\n      <td>-1.536850</td>\n      <td>0.388101</td>\n      <td>2.205761</td>\n      <td>-91.610827</td>\n    </tr>\n    <tr>\n      <th>733076</th>\n      <td>38185</td>\n      <td>773</td>\n      <td>56</td>\n      <td>-0.826121</td>\n      <td>-2.468487</td>\n      <td>-1.33282</td>\n      <td>0.381416</td>\n      <td>0.144745</td>\n      <td>1.060583</td>\n      <td>-0.765938</td>\n      <td>0.288550</td>\n      <td>0.2</td>\n      <td>-1.956647</td>\n      <td>-0.032158</td>\n      <td>-1.794239</td>\n      <td>72.414749</td>\n    </tr>\n    <tr>\n      <th>733077</th>\n      <td>38185</td>\n      <td>773</td>\n      <td>57</td>\n      <td>0.755023</td>\n      <td>1.469538</td>\n      <td>-1.33282</td>\n      <td>-1.253097</td>\n      <td>-0.414802</td>\n      <td>0.007479</td>\n      <td>0.907104</td>\n      <td>-1.556489</td>\n      <td>0.4</td>\n      <td>4.341763</td>\n      <td>0.150273</td>\n      <td>0.641975</td>\n      <td>-34.065644</td>\n    </tr>\n    <tr>\n      <th>733078</th>\n      <td>38185</td>\n      <td>773</td>\n      <td>58</td>\n      <td>-0.187017</td>\n      <td>0.714286</td>\n      <td>-1.33282</td>\n      <td>0.077876</td>\n      <td>1.323245</td>\n      <td>0.159312</td>\n      <td>-0.397996</td>\n      <td>0.306870</td>\n      <td>0.1</td>\n      <td>-1.013728</td>\n      <td>-0.608616</td>\n      <td>0.317901</td>\n      <td>65.659420</td>\n    </tr>\n    <tr>\n      <th>733079</th>\n      <td>38185</td>\n      <td>773</td>\n      <td>59</td>\n      <td>-0.414992</td>\n      <td>-2.858193</td>\n      <td>-1.33282</td>\n      <td>1.061062</td>\n      <td>-0.264150</td>\n      <td>-0.449514</td>\n      <td>-0.601093</td>\n      <td>1.621374</td>\n      <td>-1.0</td>\n      <td>-3.650289</td>\n      <td>-0.147107</td>\n      <td>-1.559671</td>\n      <td>57.189685</td>\n    </tr>\n  </tbody>\n</table>\n<p>733080 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539b752",
   "metadata": {},
   "source": [
    "由上表與題目提示可得知，每一秒會記錄一列，每 60 列代表一個 `sequence`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a76d87",
   "metadata": {},
   "source": [
    "確認資料是否有缺漏，本題的資料都很完美，沒有有缺漏的地方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68ca8905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1558080 entries, 0 to 1558079\n",
      "Data columns (total 16 columns):\n",
      " #   Column     Non-Null Count    Dtype  \n",
      "---  ------     --------------    -----  \n",
      " 0   sequence   1558080 non-null  int64  \n",
      " 1   subject    1558080 non-null  int64  \n",
      " 2   step       1558080 non-null  int64  \n",
      " 3   sensor_00  1558080 non-null  float64\n",
      " 4   sensor_01  1558080 non-null  float64\n",
      " 5   sensor_02  1558080 non-null  float64\n",
      " 6   sensor_03  1558080 non-null  float64\n",
      " 7   sensor_04  1558080 non-null  float64\n",
      " 8   sensor_05  1558080 non-null  float64\n",
      " 9   sensor_06  1558080 non-null  float64\n",
      " 10  sensor_07  1558080 non-null  float64\n",
      " 11  sensor_08  1558080 non-null  float64\n",
      " 12  sensor_09  1558080 non-null  float64\n",
      " 13  sensor_10  1558080 non-null  float64\n",
      " 14  sensor_11  1558080 non-null  float64\n",
      " 15  sensor_12  1558080 non-null  float64\n",
      "dtypes: float64(13), int64(3)\n",
      "memory usage: 190.2 MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b4db6",
   "metadata": {},
   "source": [
    "## 特徵工程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e857522f",
   "metadata": {},
   "source": [
    "萃取需要的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a79acd11",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "groups = train['sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4242d2e8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         sequence  subject  step  sensor_00  sensor_01  sensor_02  sensor_03  \\\n0               0       47     0  -0.196291   0.112395   1.000000   0.329204   \n1               0       47     1  -0.447450   0.134454   1.000000  -0.658407   \n2               0       47     2   0.326893  -0.694328   1.000000   0.330088   \n3               0       47     3   0.523184   0.751050   1.000000   0.976991   \n4               0       47     4   0.272025   1.074580   1.000000  -0.136283   \n...           ...      ...   ...        ...        ...        ...        ...   \n1558075     25967      327    55  -0.282844  -1.217437  -1.666153   0.586726   \n1558076     25967      327    56   0.130603   0.349790  -1.666153  -0.324779   \n1558077     25967      327    57  -0.579598   0.429622  -1.666153   0.319469   \n1558078     25967      327    58   1.278980   1.711134  -1.522820   0.802655   \n1558079     25967      327    59  -1.136012  -3.702731  -1.332820  -0.766372   \n\n         sensor_04  sensor_05  sensor_06  sensor_07  sensor_08  sensor_09  \\\n0        -1.004660  -0.131638  -0.127505   0.368702       -0.1  -0.963873   \n1         0.162495   0.340314  -0.209472  -0.867176        0.2  -0.301301   \n2         0.473678   1.280479  -0.094718   0.535878        1.4   1.002168   \n3        -0.563287  -0.720269   0.793260   0.951145       -0.3  -0.995665   \n4         0.398579   0.044877   0.560109  -0.541985       -0.9   1.055636   \n...            ...        ...        ...        ...        ...        ...   \n1558075  -0.930698  -0.451010  -0.651184   0.368702        0.4   0.008671   \n1558076   0.775324  -0.332835   0.099271   0.122137       -0.2   0.644509   \n1558077   0.308861   0.282723  -0.512750   0.012214       -1.6  -0.424133   \n1558078  -0.460541  -0.055348   2.405282   0.043511        1.9   0.283960   \n1558079  -0.430027  -0.091997  -2.512750  -0.022901       -1.1  -0.653902   \n\n         sensor_10  sensor_11  sensor_12  \n0        -0.985069   0.531893   4.751492  \n1         0.082733  -0.231481   0.454390  \n2         0.449221  -0.586420  -4.736147  \n3        -0.434290   1.344650   0.429241  \n4         0.812631   0.123457  -0.223359  \n...            ...        ...        ...  \n1558075  -0.723536  -0.353909  -0.914749  \n1558076   0.691407  -0.613169  -0.515772  \n1558077   0.716855   1.628601   0.928389  \n1558078  -0.914914   0.364198   0.211424  \n1558079  -0.418516  -1.453704  -1.561381  \n\n[1558080 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sequence</th>\n      <th>subject</th>\n      <th>step</th>\n      <th>sensor_00</th>\n      <th>sensor_01</th>\n      <th>sensor_02</th>\n      <th>sensor_03</th>\n      <th>sensor_04</th>\n      <th>sensor_05</th>\n      <th>sensor_06</th>\n      <th>sensor_07</th>\n      <th>sensor_08</th>\n      <th>sensor_09</th>\n      <th>sensor_10</th>\n      <th>sensor_11</th>\n      <th>sensor_12</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>47</td>\n      <td>0</td>\n      <td>-0.196291</td>\n      <td>0.112395</td>\n      <td>1.000000</td>\n      <td>0.329204</td>\n      <td>-1.004660</td>\n      <td>-0.131638</td>\n      <td>-0.127505</td>\n      <td>0.368702</td>\n      <td>-0.1</td>\n      <td>-0.963873</td>\n      <td>-0.985069</td>\n      <td>0.531893</td>\n      <td>4.751492</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>47</td>\n      <td>1</td>\n      <td>-0.447450</td>\n      <td>0.134454</td>\n      <td>1.000000</td>\n      <td>-0.658407</td>\n      <td>0.162495</td>\n      <td>0.340314</td>\n      <td>-0.209472</td>\n      <td>-0.867176</td>\n      <td>0.2</td>\n      <td>-0.301301</td>\n      <td>0.082733</td>\n      <td>-0.231481</td>\n      <td>0.454390</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>47</td>\n      <td>2</td>\n      <td>0.326893</td>\n      <td>-0.694328</td>\n      <td>1.000000</td>\n      <td>0.330088</td>\n      <td>0.473678</td>\n      <td>1.280479</td>\n      <td>-0.094718</td>\n      <td>0.535878</td>\n      <td>1.4</td>\n      <td>1.002168</td>\n      <td>0.449221</td>\n      <td>-0.586420</td>\n      <td>-4.736147</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>47</td>\n      <td>3</td>\n      <td>0.523184</td>\n      <td>0.751050</td>\n      <td>1.000000</td>\n      <td>0.976991</td>\n      <td>-0.563287</td>\n      <td>-0.720269</td>\n      <td>0.793260</td>\n      <td>0.951145</td>\n      <td>-0.3</td>\n      <td>-0.995665</td>\n      <td>-0.434290</td>\n      <td>1.344650</td>\n      <td>0.429241</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>47</td>\n      <td>4</td>\n      <td>0.272025</td>\n      <td>1.074580</td>\n      <td>1.000000</td>\n      <td>-0.136283</td>\n      <td>0.398579</td>\n      <td>0.044877</td>\n      <td>0.560109</td>\n      <td>-0.541985</td>\n      <td>-0.9</td>\n      <td>1.055636</td>\n      <td>0.812631</td>\n      <td>0.123457</td>\n      <td>-0.223359</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1558075</th>\n      <td>25967</td>\n      <td>327</td>\n      <td>55</td>\n      <td>-0.282844</td>\n      <td>-1.217437</td>\n      <td>-1.666153</td>\n      <td>0.586726</td>\n      <td>-0.930698</td>\n      <td>-0.451010</td>\n      <td>-0.651184</td>\n      <td>0.368702</td>\n      <td>0.4</td>\n      <td>0.008671</td>\n      <td>-0.723536</td>\n      <td>-0.353909</td>\n      <td>-0.914749</td>\n    </tr>\n    <tr>\n      <th>1558076</th>\n      <td>25967</td>\n      <td>327</td>\n      <td>56</td>\n      <td>0.130603</td>\n      <td>0.349790</td>\n      <td>-1.666153</td>\n      <td>-0.324779</td>\n      <td>0.775324</td>\n      <td>-0.332835</td>\n      <td>0.099271</td>\n      <td>0.122137</td>\n      <td>-0.2</td>\n      <td>0.644509</td>\n      <td>0.691407</td>\n      <td>-0.613169</td>\n      <td>-0.515772</td>\n    </tr>\n    <tr>\n      <th>1558077</th>\n      <td>25967</td>\n      <td>327</td>\n      <td>57</td>\n      <td>-0.579598</td>\n      <td>0.429622</td>\n      <td>-1.666153</td>\n      <td>0.319469</td>\n      <td>0.308861</td>\n      <td>0.282723</td>\n      <td>-0.512750</td>\n      <td>0.012214</td>\n      <td>-1.6</td>\n      <td>-0.424133</td>\n      <td>0.716855</td>\n      <td>1.628601</td>\n      <td>0.928389</td>\n    </tr>\n    <tr>\n      <th>1558078</th>\n      <td>25967</td>\n      <td>327</td>\n      <td>58</td>\n      <td>1.278980</td>\n      <td>1.711134</td>\n      <td>-1.522820</td>\n      <td>0.802655</td>\n      <td>-0.460541</td>\n      <td>-0.055348</td>\n      <td>2.405282</td>\n      <td>0.043511</td>\n      <td>1.9</td>\n      <td>0.283960</td>\n      <td>-0.914914</td>\n      <td>0.364198</td>\n      <td>0.211424</td>\n    </tr>\n    <tr>\n      <th>1558079</th>\n      <td>25967</td>\n      <td>327</td>\n      <td>59</td>\n      <td>-1.136012</td>\n      <td>-3.702731</td>\n      <td>-1.332820</td>\n      <td>-0.766372</td>\n      <td>-0.430027</td>\n      <td>-0.091997</td>\n      <td>-2.512750</td>\n      <td>-0.022901</td>\n      <td>-1.1</td>\n      <td>-0.653902</td>\n      <td>-0.418516</td>\n      <td>-1.453704</td>\n      <td>-1.561381</td>\n    </tr>\n  </tbody>\n</table>\n<p>1558080 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5fa550",
   "metadata": {},
   "outputs": [],
   "source": [
    "Window = 60\n",
    "y = train_labels['state'].to_numpy()\n",
    "train = train.loc[:, ['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04', 'sensor_05', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09', 'sensor_10', 'sensor_11', 'sensor_12']]\n",
    "test = test.loc[:, ['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04', 'sensor_05', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09', 'sensor_10', 'sensor_11', 'sensor_12']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "141a0200",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 1, ..., 1, 1, 0], dtype=int64)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cfd4cfa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(25968,)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "012db34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(train)\n",
    "X_train = sc.transform(train)\n",
    "X_test = sc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84864509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.07399419,  0.02575496,  0.52994079, ..., -0.51394323,\n         0.11817284,  0.12176644],\n       [-0.16846171,  0.03076355,  0.52994079, ...,  0.0429771 ,\n        -0.05024707,  0.01191576],\n       [ 0.1227889 , -0.15741634,  0.52994079, ...,  0.23412207,\n        -0.12855551, -0.12077463],\n       ...,\n       [-0.21816616,  0.09778326, -0.63026452, ...,  0.37370821,\n         0.36013458,  0.024033  ],\n       [ 0.48089345,  0.38875849, -0.56789148, ..., -0.47735358,\n         0.08117493,  0.00570458],\n       [-0.42744803, -0.84049264, -0.48521092, ..., -0.21845335,\n        -0.31990049, -0.03961523]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f682763",
   "metadata": {},
   "source": [
    "## 整理資料"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb254ea5",
   "metadata": {},
   "source": [
    "準備進入訓練的資料格式：設定 sequence\n",
    "sequence 設為 60 ，代表過去 60 秒的資訊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c419e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y.copy()\n",
    "X_train = X_train.reshape(-1, Window, X_train.shape[-1])\n",
    "X_test = X_test.reshape(-1, Window, X_test.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee0db4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25968,) (25968, 60, 13) (12218, 60, 13)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "064647df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[-7.39941949e-02,  2.57549585e-02,  5.29940790e-01, ...,\n         -5.13943234e-01,  1.18172836e-01,  1.21766440e-01],\n        [-1.68461709e-01,  3.07635486e-02,  5.29940790e-01, ...,\n          4.29771001e-02, -5.02470657e-02,  1.19157570e-02],\n        [ 1.22788905e-01, -1.57416339e-01,  5.29940790e-01, ...,\n          2.34122074e-01, -1.28555511e-01, -1.20774628e-01],\n        ...,\n        [ 3.66369758e-01, -5.87678086e-01,  3.84887188e-01, ...,\n          9.43643246e-01, -6.54924449e-01, -1.84052370e-01],\n        [-2.11943389e+00,  8.23074813e-01,  3.84887188e-01, ...,\n          2.32894843e-01,  2.98395750e-01, -6.83245867e-01],\n        [ 2.83357057e+00, -4.06891831e-01,  3.84887188e-01, ...,\n         -8.30538459e-01,  3.22682717e-01, -1.29474183e+00]],\n\n       [[-2.50428001e+00, -3.19630807e-02, -9.20148225e-01, ...,\n          6.09078938e-01,  3.37209501e-01, -3.17886919e-01],\n        [ 6.14310650e-01,  1.33320395e-01, -8.31077714e-01, ...,\n          6.16109001e-01,  5.27419578e-01,  2.16022071e+00],\n        [ 7.00639425e-01, -4.86552265e-01, -7.75318126e-01, ...,\n         -1.00100232e+00, -7.73862492e-01, -9.18420696e-01],\n        ...,\n        [ 4.24794282e-01,  1.23541719e-01, -4.85210922e-01, ...,\n          1.90926582e-01,  2.17817495e-01, -4.89340139e-05],\n        [-2.63244336e-02, -9.57522458e-04, -4.85210922e-01, ...,\n          2.21607350e-01, -9.22385506e-02,  2.80602869e-03],\n        [-3.88232125e-02, -1.77927709e-01, -3.40882588e-01, ...,\n         -1.63239007e-02, -1.67823223e-01, -2.64237341e-03]],\n\n       [[-6.55332746e-01,  3.55336345e-02, -3.40157320e-01, ...,\n          3.56981505e-01, -1.74178691e-01,  4.08095478e-03],\n        [ 1.27965262e+00, -3.80179350e-01, -3.40157320e-01, ...,\n          1.88214554e-01,  7.11877691e-02,  5.72183826e-04],\n        [-2.37931666e-01,  5.27329489e-01, -3.40157320e-01, ...,\n         -3.63781706e-01,  1.65384884e-01,  4.38606530e-03],\n        ...,\n        [-3.20772410e-01, -1.67672024e-01, -4.85210922e-01, ...,\n          6.36733615e-02, -9.65511896e-02, -2.03215238e-03],\n        [ 2.19266661e-02, -9.85057787e-02, -4.85210922e-01, ...,\n          2.92453411e-01, -7.02213937e-02, -7.54593531e-03],\n        [ 2.80622322e-01,  5.84300468e-02, -4.85210922e-01, ...,\n         -1.82666693e-01,  4.98515551e-02,  1.47661857e-03]],\n\n       ...,\n\n       [[-3.67885276e-02,  4.06169307e-01, -1.95103717e-01, ...,\n          1.15540873e+00,  2.02155806e-01,  1.64270339e-02],\n        [-3.23969772e-01, -4.24572696e-02, -1.95103717e-01, ...,\n         -1.04247060e+00, -1.64191527e-01, -1.02592396e-02],\n        [ 3.49220271e-01, -4.26926192e-01, -3.29278299e-01, ...,\n          1.12366739e+00, -1.08127221e-01,  3.26369446e-03],\n        ...,\n        [-5.65225271e-01, -9.96983002e-02, -2.33542922e-01, ...,\n         -1.32177922e+00, -1.50118706e-01,  1.85301172e-02],\n        [ 2.34696576e-01,  2.48756473e-01, -3.40157320e-01, ...,\n          3.60996519e-01, -4.23027307e-02, -3.49405011e-02],\n        [ 1.77725398e-01,  1.13763043e-01, -3.40157320e-01, ...,\n          7.37771504e-01,  5.13119775e-01,  4.63714519e-02]],\n\n       [[-1.34453404e-01,  1.40714028e-01, -1.95103717e-01, ...,\n          1.28594971e+00, -4.75232937e-02, -2.31497055e-01],\n        [-6.03302946e-01,  3.37241566e-01, -1.95103717e-01, ...,\n         -5.45623968e-01, -1.25682198e-02, -2.50893367e-01],\n        [-1.11781201e-01, -6.03657873e-01, -1.95103717e-01, ...,\n         -1.54848359e+00, -3.23155667e-02, -5.52832914e-01],\n        ...,\n        [-2.44035721e-01, -2.17280917e-01,  9.47799837e-02, ...,\n          7.64497601e-03,  6.95989021e-02, -3.90503222e-01],\n        [-2.25457331e-02,  1.52607695e-02,  9.47799837e-02, ...,\n          7.38165429e-01, -1.58517003e-01, -2.72207516e-01],\n        [ 2.54752756e-01,  7.82259032e-02,  9.47799837e-02, ...,\n          1.47820071e+00, -3.94294177e-03,  4.97183139e-01]],\n\n       [[-1.18175925e-01, -2.19459003e-02, -6.30264524e-01, ...,\n         -2.02984179e-01,  5.96117381e-02, -9.80157378e-03],\n        [ 1.47495793e-01, -1.01129326e-01, -6.30264524e-01, ...,\n         -2.42755545e-01, -2.82902590e-01, -3.33495677e-02],\n        [-2.08574070e-01,  2.66405791e-01,  2.40057089e-01, ...,\n          2.95483610e-01,  2.35068051e-01,  4.37017349e-02],\n        ...,\n        [-2.18166156e-01,  9.77832553e-02, -6.30264524e-01, ...,\n          3.73708206e-01,  3.60134582e-01,  2.40330033e-02],\n        [ 4.80893453e-01,  3.88758494e-01, -5.67891475e-01, ...,\n         -4.77353577e-01,  8.11749331e-02,  5.70457861e-03],\n        [-4.27448034e-01, -8.40492637e-01, -4.85210922e-01, ...,\n         -2.18453347e-01, -3.19900493e-01, -3.96152301e-02]]])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d86d50",
   "metadata": {},
   "source": [
    "## 搭建 LSTM 網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb26608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.metrics import AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0196d941",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DNN_Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 60, 13)]          0         \n",
      "                                                                 \n",
      " bidirectional_39 (Bidirecti  (None, 60, 256)          145408    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_40 (Bidirecti  (None, 60, 128)          164352    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_41 (Bidirecti  (None, 60, 128)          98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_42 (Bidirecti  (None, 60, 64)           41216     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 60, 128)           8320      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 60, 128)           0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 7680)              0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 7681      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 465,793\n",
      "Trainable params: 465,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    input_layer = Input(shape=(X_train.shape[1:]))\n",
    "\n",
    "\n",
    "    x1 = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\n",
    "    x21 = Bidirectional(LSTM(64, return_sequences=True))(x1)\n",
    "    # x22 = Bidirectional(LSTM(32, return_sequences=True))(input_layer)\n",
    "    # l2 = Concatenate(axis=2)([x21, x22])\n",
    "\n",
    "    # x31 = Bidirectional(LSTM(16, return_sequences=True))(l2)\n",
    "    x32 = Bidirectional(LSTM(64, return_sequences=True))(x21)\n",
    "    # l3 = Concatenate(axis=2)([x31, x32])\n",
    "\n",
    "    # x41 = Bidirectional(LSTM(8, return_sequences=True))(l3)\n",
    "    x42 = Bidirectional(LSTM(32, return_sequences=True))(x32)\n",
    "    # l4 = Concatenate(axis=2)([x41, x42])\n",
    "\n",
    "    # l5 = Concatenate(axis=2)([x1, l2, l3, l4])\n",
    "    x7 = Dense(128, activation='selu')(x42)\n",
    "    x8 = Dropout(0.5)(x7)\n",
    "    f = Flatten()(x8)\n",
    "    output_layer = Dense(units=1, activation=\"sigmoid\")(f)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer, name='DNN_Model')\n",
    "    model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\", metrics=[AUC(name = 'auc')])\n",
    "    return  model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033709d1",
   "metadata": {},
   "source": [
    "Reson for why add Fallten in last dense\n",
    "\n",
    "An LSTM layer consists of different LSTM cells that are processed sequentially. As seen in the figure below, the first cell takes an input/embedding calculates a hidden state and the next cell uses its input and the hidden state at previous time step to compute its own hidden state. Basically the arrows between the cells also pass the hidden states. <b>If you do return_sequences=False, the lstm layer only outputs the very last hidden state! (h_4 in the figure). So, all those information from all inputs and cells are embedded in a single fixed size information and it can not contain lots of information.</b> This is why, your accuracy is not good when you only use the last hidden state.\n",
    "\n",
    "When you do `return_sequences=True`, lstm layer outputs every hidden state, so the next layers have access to all hidden states and they contain naturally more information. However, the LSTM layer returns a matrix. You can also see this in your model summary. It returns a matrix of size (None, 500, 128). None is basically number of samples in your batch, you can forget about it. 500 is your input size, and 128 is your hidden state size. <b>The dense layer can not process a matrix, it has to be a vector. That why you need to apply flatten and what it does is basically just to open up the 2D matrix and represent it as 1D vector.</b> Therefore, the size of your Flatten layer is 64000 because 500*128 = 64000. And Of course with more hidden states, the accuracy is better as they contain more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8be3fba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "833410b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(hist, metric='auc', ax=None, fold=0):\n",
    "    if ax==None:\n",
    "        plt.plot(hist.history[metric])\n",
    "        plt.plot(hist.history[\"val_\" + metric])\n",
    "        plt.title(f\"model performance fold {fold}\")\n",
    "        plt.ylabel(\"area_under_curve\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "        plt.show()\n",
    "        return\n",
    "    else:\n",
    "        ax.plot(hist.history[metric])\n",
    "        ax.plot(hist.history[\"val_\" + metric])\n",
    "        ax.set_title(f\"model performance fold {fold}\")\n",
    "        ax.set_ylabel(\"area_under_curve\")\n",
    "        ax.set_xlabel(\"epoch\")\n",
    "        ax.legend([\"train\", \"validation\"], loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4258179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Fold: 1 Epoch 1/60\n",
      "609/609 [==============================] - 38s 49ms/step - loss: 0.6393 - auc: 0.6778 - val_loss: 0.5124 - val_auc: 0.8378 - lr: 0.0010\n",
      "Epoch 2/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.4717 - auc: 0.8569 - val_loss: 0.4312 - val_auc: 0.8870 - lr: 0.0010\n",
      "Epoch 3/60\n",
      "609/609 [==============================] - 29s 48ms/step - loss: 0.3929 - auc: 0.9035 - val_loss: 0.3753 - val_auc: 0.9141 - lr: 0.0010\n",
      "Epoch 4/60\n",
      "609/609 [==============================] - 30s 49ms/step - loss: 0.3418 - auc: 0.9279 - val_loss: 0.3687 - val_auc: 0.9247 - lr: 0.0010\n",
      "Epoch 5/60\n",
      "609/609 [==============================] - 30s 49ms/step - loss: 0.3046 - auc: 0.9431 - val_loss: 0.3238 - val_auc: 0.9361 - lr: 0.0010\n",
      "Epoch 6/60\n",
      "609/609 [==============================] - 30s 49ms/step - loss: 0.2831 - auc: 0.9510 - val_loss: 0.3017 - val_auc: 0.9444 - lr: 0.0010\n",
      "Epoch 7/60\n",
      "609/609 [==============================] - 29s 48ms/step - loss: 0.2554 - auc: 0.9601 - val_loss: 0.3116 - val_auc: 0.9415 - lr: 0.0010\n",
      "Epoch 8/60\n",
      "609/609 [==============================] - 30s 49ms/step - loss: 0.2299 - auc: 0.9677 - val_loss: 0.2853 - val_auc: 0.9515 - lr: 0.0010\n",
      "Epoch 9/60\n",
      "609/609 [==============================] - 30s 49ms/step - loss: 0.2099 - auc: 0.9731 - val_loss: 0.3063 - val_auc: 0.9518 - lr: 0.0010\n",
      "Epoch 10/60\n",
      "609/609 [==============================] - 30s 49ms/step - loss: 0.1939 - auc: 0.9769 - val_loss: 0.2909 - val_auc: 0.9528 - lr: 0.0010\n",
      "Epoch 11/60\n",
      "609/609 [==============================] - 30s 49ms/step - loss: 0.1761 - auc: 0.9810 - val_loss: 0.2983 - val_auc: 0.9512 - lr: 0.0010\n",
      "Epoch 12/60\n",
      "609/609 [==============================] - 29s 48ms/step - loss: 0.1558 - auc: 0.9849 - val_loss: 0.3178 - val_auc: 0.9526 - lr: 0.0010\n",
      "Epoch 13/60\n",
      "609/609 [==============================] - 30s 49ms/step - loss: 0.1400 - auc: 0.9877 - val_loss: 0.3690 - val_auc: 0.9452 - lr: 0.0010\n",
      "Epoch 14/60\n",
      "609/609 [==============================] - 30s 49ms/step - loss: 0.1295 - auc: 0.9895 - val_loss: 0.3762 - val_auc: 0.9481 - lr: 0.0010\n",
      "Epoch 15/60\n",
      "609/609 [==============================] - 29s 48ms/step - loss: 0.0880 - auc: 0.9949 - val_loss: 0.4448 - val_auc: 0.9452 - lr: 7.0000e-04\n",
      "Epoch 16/60\n",
      "609/609 [==============================] - 30s 50ms/step - loss: 0.0734 - auc: 0.9961 - val_loss: 0.4124 - val_auc: 0.9442 - lr: 7.0000e-04\n",
      "Epoch 17/60\n",
      "609/609 [==============================] - 31s 51ms/step - loss: 0.0697 - auc: 0.9967 - val_loss: 0.5083 - val_auc: 0.9364 - lr: 7.0000e-04\n",
      "Epoch 18/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.0669 - auc: 0.9968 - val_loss: 0.5091 - val_auc: 0.9385 - lr: 7.0000e-04\n",
      "Epoch 19/60\n",
      "609/609 [==============================] - 29s 48ms/step - loss: 0.0385 - auc: 0.9986 - val_loss: 0.5934 - val_auc: 0.9353 - lr: 4.9000e-04\n",
      "Epoch 20/60\n",
      "609/609 [==============================] - 29s 48ms/step - loss: 0.0348 - auc: 0.9990 - val_loss: 0.5914 - val_auc: 0.9317 - lr: 4.9000e-04\n",
      "auc: 0.95338\n",
      "Fold: 2 Epoch 1/60\n",
      "609/609 [==============================] - 40s 51ms/step - loss: 0.6071 - auc: 0.7297 - val_loss: 0.5023 - val_auc: 0.8344 - lr: 0.0010\n",
      "Epoch 2/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.4627 - auc: 0.8627 - val_loss: 0.3954 - val_auc: 0.9028 - lr: 0.0010\n",
      "Epoch 3/60\n",
      "609/609 [==============================] - 28s 47ms/step - loss: 0.3741 - auc: 0.9133 - val_loss: 0.3468 - val_auc: 0.9328 - lr: 0.0010\n",
      "Epoch 4/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.3259 - auc: 0.9347 - val_loss: 0.3153 - val_auc: 0.9438 - lr: 0.0010\n",
      "Epoch 5/60\n",
      "609/609 [==============================] - 27s 45ms/step - loss: 0.2888 - auc: 0.9490 - val_loss: 0.3021 - val_auc: 0.9461 - lr: 0.0010\n",
      "Epoch 6/60\n",
      "609/609 [==============================] - 28s 46ms/step - loss: 0.2636 - auc: 0.9576 - val_loss: 0.2835 - val_auc: 0.9523 - lr: 0.0010\n",
      "Epoch 7/60\n",
      "609/609 [==============================] - 28s 46ms/step - loss: 0.2417 - auc: 0.9644 - val_loss: 0.2676 - val_auc: 0.9569 - lr: 0.0010\n",
      "Epoch 8/60\n",
      "609/609 [==============================] - 29s 48ms/step - loss: 0.2215 - auc: 0.9700 - val_loss: 0.3002 - val_auc: 0.9556 - lr: 0.0010\n",
      "Epoch 9/60\n",
      "609/609 [==============================] - 28s 47ms/step - loss: 0.1978 - auc: 0.9759 - val_loss: 0.2963 - val_auc: 0.9554 - lr: 0.0010\n",
      "Epoch 10/60\n",
      "609/609 [==============================] - 28s 47ms/step - loss: 0.1842 - auc: 0.9791 - val_loss: 0.2990 - val_auc: 0.9591 - lr: 0.0010\n",
      "Epoch 11/60\n",
      "609/609 [==============================] - 29s 48ms/step - loss: 0.1663 - auc: 0.9829 - val_loss: 0.3034 - val_auc: 0.9519 - lr: 0.0010\n",
      "Epoch 12/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.1527 - auc: 0.9855 - val_loss: 0.3356 - val_auc: 0.9475 - lr: 0.0010\n",
      "Epoch 13/60\n",
      "609/609 [==============================] - 29s 48ms/step - loss: 0.1334 - auc: 0.9886 - val_loss: 0.3482 - val_auc: 0.9474 - lr: 0.0010\n",
      "Epoch 14/60\n",
      "609/609 [==============================] - 29s 48ms/step - loss: 0.1258 - auc: 0.9899 - val_loss: 0.3230 - val_auc: 0.9536 - lr: 0.0010\n",
      "Epoch 15/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.0793 - auc: 0.9957 - val_loss: 0.3987 - val_auc: 0.9480 - lr: 7.0000e-04\n",
      "Epoch 16/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.0675 - auc: 0.9967 - val_loss: 0.4470 - val_auc: 0.9464 - lr: 7.0000e-04\n",
      "Epoch 17/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.0599 - auc: 0.9975 - val_loss: 0.5239 - val_auc: 0.9395 - lr: 7.0000e-04\n",
      "Epoch 18/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.0535 - auc: 0.9977 - val_loss: 0.5387 - val_auc: 0.9382 - lr: 7.0000e-04\n",
      "Epoch 19/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.0303 - auc: 0.9992 - val_loss: 0.5780 - val_auc: 0.9386 - lr: 4.9000e-04\n",
      "Epoch 20/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.0206 - auc: 0.9996 - val_loss: 0.6869 - val_auc: 0.9324 - lr: 4.9000e-04\n",
      "auc: 0.96007\n",
      "Fold: 3 Epoch 1/60\n",
      "609/609 [==============================] - 40s 52ms/step - loss: 0.6377 - auc: 0.6785 - val_loss: 0.5242 - val_auc: 0.8204 - lr: 0.0010\n",
      "Epoch 2/60\n",
      "609/609 [==============================] - 28s 47ms/step - loss: 0.4723 - auc: 0.8559 - val_loss: 0.4099 - val_auc: 0.8946 - lr: 0.0010\n",
      "Epoch 3/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.3834 - auc: 0.9082 - val_loss: 0.3813 - val_auc: 0.9114 - lr: 0.0010\n",
      "Epoch 4/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.3380 - auc: 0.9297 - val_loss: 0.3507 - val_auc: 0.9311 - lr: 0.0010\n",
      "Epoch 5/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.3028 - auc: 0.9437 - val_loss: 0.3252 - val_auc: 0.9408 - lr: 0.0010\n",
      "Epoch 6/60\n",
      "609/609 [==============================] - 29s 48ms/step - loss: 0.2736 - auc: 0.9543 - val_loss: 0.3238 - val_auc: 0.9371 - lr: 0.0010\n",
      "Epoch 7/60\n",
      "609/609 [==============================] - 29s 48ms/step - loss: 0.2550 - auc: 0.9603 - val_loss: 0.3050 - val_auc: 0.9460 - lr: 0.0010\n",
      "Epoch 8/60\n",
      "609/609 [==============================] - 29s 48ms/step - loss: 0.2302 - auc: 0.9676 - val_loss: 0.3102 - val_auc: 0.9444 - lr: 0.0010\n",
      "Epoch 9/60\n",
      "609/609 [==============================] - 28s 46ms/step - loss: 0.2138 - auc: 0.9720 - val_loss: 0.3053 - val_auc: 0.9464 - lr: 0.0010\n",
      "Epoch 10/60\n",
      "609/609 [==============================] - 28s 46ms/step - loss: 0.1970 - auc: 0.9761 - val_loss: 0.3131 - val_auc: 0.9521 - lr: 0.0010\n",
      "Epoch 11/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.1784 - auc: 0.9803 - val_loss: 0.3149 - val_auc: 0.9481 - lr: 0.0010\n",
      "Epoch 12/60\n",
      "609/609 [==============================] - 28s 46ms/step - loss: 0.1604 - auc: 0.9841 - val_loss: 0.3060 - val_auc: 0.9492 - lr: 0.0010\n",
      "Epoch 13/60\n",
      "609/609 [==============================] - 28s 46ms/step - loss: 0.1496 - auc: 0.9860 - val_loss: 0.3462 - val_auc: 0.9457 - lr: 0.0010\n",
      "Epoch 14/60\n",
      "609/609 [==============================] - 29s 47ms/step - loss: 0.1308 - auc: 0.9892 - val_loss: 0.3757 - val_auc: 0.9404 - lr: 0.0010\n",
      "Epoch 15/60\n",
      "609/609 [==============================] - 28s 45ms/step - loss: 0.0957 - auc: 0.9938 - val_loss: 0.4043 - val_auc: 0.9424 - lr: 7.0000e-04\n",
      "Epoch 16/60\n",
      "609/609 [==============================] - 28s 47ms/step - loss: 0.0813 - auc: 0.9953 - val_loss: 0.4315 - val_auc: 0.9410 - lr: 7.0000e-04\n",
      "Epoch 17/60\n",
      "485/609 [======================>.......] - ETA: 5s - loss: 0.0687 - auc: 0.9968"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11360/1556571552.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[0mfolds\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 46\u001B[1;33m \u001B[1;33m(\u001B[0m\u001B[0mtest_preds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mauc\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfit_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfolds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m60\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     47\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_preds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11360/1556571552.py\u001B[0m in \u001B[0;36mfit_model\u001B[1;34m(folds, epochs, batch_size, verbose)\u001B[0m\n\u001B[0;32m     23\u001B[0m         \u001B[0mlr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mReduceLROnPlateau\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmonitor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"val_auc\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'max'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfactor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.7\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpatience\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m         \u001B[0mes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mEarlyStopping\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmonitor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'val_auc'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'max'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpatience\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mrestore_best_weights\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m         history = model.fit(X_train_part, y_train_part, validation_data=(X_valid, y_valid), epochs=epochs, batch_size=batch_size,\n\u001B[0m\u001B[0;32m     26\u001B[0m                             callbacks=[es,lr], verbose=verbose)\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\tensor_boshi\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 64\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     65\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\tensor_boshi\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1382\u001B[0m                 _r=1):\n\u001B[0;32m   1383\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1384\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1385\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1386\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\tensor_boshi\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    149\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 150\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\tensor_boshi\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    913\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    914\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 915\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    916\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    917\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\tensor_boshi\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    945\u001B[0m       \u001B[1;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    946\u001B[0m       \u001B[1;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 947\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=not-callable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    948\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    949\u001B[0m       \u001B[1;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\tensor_boshi\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2954\u001B[0m       (graph_function,\n\u001B[0;32m   2955\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2956\u001B[1;33m     return graph_function._call_flat(\n\u001B[0m\u001B[0;32m   2957\u001B[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0;32m   2958\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\tensor_boshi\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1851\u001B[0m         and executing_eagerly):\n\u001B[0;32m   1852\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1853\u001B[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0m\u001B[0;32m   1854\u001B[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0;32m   1855\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001B[1;32m~\\.conda\\envs\\tensor_boshi\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0m_InterpolateFunctionError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    498\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcancellation_manager\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 499\u001B[1;33m           outputs = execute.execute(\n\u001B[0m\u001B[0;32m    500\u001B[0m               \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    501\u001B[0m               \u001B[0mnum_outputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_outputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\tensor_boshi\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 54\u001B[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[0;32m     55\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[0;32m     56\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1152x288 with 4 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "\n",
    "def fit_model(folds, epochs=60, batch_size=32, verbose=True):\n",
    "    test_preds = []\n",
    "    auc = []\n",
    "    nfold = folds\n",
    "    ncols = 5 if folds > 5 else folds\n",
    "    nrows = int(round(nfold / ncols))\n",
    "\n",
    "    col, row = 0, 0\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(16, round(nrows*16/ncols)))\n",
    "    print(nrows)\n",
    "\n",
    "    kf = GroupKFold(n_splits=nfold)\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X_train, y_train, groups.unique())):\n",
    "        print(f\"Fold: {fold+1}\", end=' ')\n",
    "        X_train_part, X_valid = X_train[train_idx], X_train[test_idx]\n",
    "        y_train_part, y_valid = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "        model = get_model()\n",
    "        lr = ReduceLROnPlateau(monitor=\"val_auc\", mode='max', factor=0.7, patience=4, verbose=False)\n",
    "        es = EarlyStopping(monitor='val_auc',mode='max', patience=10, verbose=False,restore_best_weights=True)\n",
    "        history = model.fit(X_train_part, y_train_part, validation_data=(X_valid, y_valid), epochs=epochs, batch_size=batch_size,\n",
    "                            callbacks=[es,lr], verbose=verbose)\n",
    "\n",
    "        y_pred = model.predict(X_valid).squeeze()\n",
    "        auc_score = roc_auc_score(y_valid, y_pred)\n",
    "        print(f'auc: {round(auc_score, 5)}')\n",
    "        test_preds.append(model.predict(X_test).squeeze())\n",
    "        auc.append(auc_score)\n",
    "        model.save('fold_{}.h5'.format(nfold))\n",
    "        plot_hist(history, metric='auc', ax=axes[col] if nrows <= 1 else axes[row][col], fold=fold+1)\n",
    "        del X_train_part, X_valid, y_train_part, y_valid, model, history\n",
    "        gc.collect()\n",
    "\n",
    "        col += 1\n",
    "        if col >= ncols:\n",
    "            row += 1\n",
    "            col = 0\n",
    "\n",
    "    return (test_preds, auc)\n",
    "\n",
    "folds = 4\n",
    "(test_preds, auc) = fit_model(folds, epochs=60, batch_size=32)\n",
    "\n",
    "print(test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3bc4c",
   "metadata": {},
   "source": [
    "## 預測真實資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b639ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"the mean AUC for the {folds} folds is : {round(np.mean(auc)*100,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e14ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data['state'] = sum(test_preds)/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f2a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data.state = (sub_data.state > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60354a78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c416366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}