{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\boshi\\GitHub\\kears_learning\\training_project\\Natural Language Processing with Disaster Tweets\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": "   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.columns)\n",
    "train.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_label = train.target.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "y_train = train_label.reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train.drop(['id', 'keyword', 'location', 'target'], axis=1, inplace=True)\n",
    "test.drop(['id', 'keyword', 'location'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train = train.to_numpy()\n",
    "test = test.to_numpy()\n",
    "X_train = train.reshape(-1)\n",
    "X_test = test.reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spilt train and valid data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "\n",
    "  #apostrophe\n",
    "  tweet = re.sub(r'\\x89Ûª', \"'\", tweet)\n",
    "  tweet = re.sub(r'\\x89Û÷', \"'\", tweet)\n",
    "\n",
    "  #quotation\n",
    "  tweet = re.sub(r'\\x89ÛÏ', '\"', tweet)\n",
    "  tweet = re.sub(r'\\x89Û\\x9d', '\"', tweet)\n",
    "\n",
    "  #hyphen\n",
    "  tweet = re.sub(r'\\x89ÛÒ', '-', tweet)\n",
    "  tweet = re.sub(r'\\x89ÛÓ', '—', tweet)\n",
    "\n",
    "  #euro\n",
    "  tweet = re.sub(r'\\x89âÂ', '€', tweet)\n",
    "\n",
    "  #ellipsis\n",
    "  tweet = re.sub(r'\\x89Û_', '...', tweet)\n",
    "\n",
    "  #amp\n",
    "  tweet = re.sub(r'&', 'and', tweet)\n",
    "\n",
    "\n",
    "  #bullet\n",
    "  tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
    "  tweet = re.sub(r'\\x89Û¢', '', tweet)\n",
    "\n",
    "\n",
    "  #no idea\n",
    "  tweet = re.sub(r'\\x89ã¢', '', tweet)\n",
    "  tweet = re.sub(r\"å_\", \"\", tweet)\n",
    "\n",
    "\n",
    "  #other\n",
    "  tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
    "  tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
    "  tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
    "  tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)\n",
    "  tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
    "  tweet = re.sub(r\"å¨\", \"\", tweet)\n",
    "  tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
    "  tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
    "  tweet = re.sub(r\"å£3million\", \"3 million \", tweet)\n",
    "  tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
    "\n",
    "  return tweet\n",
    "\n",
    "def find_URL(text):\n",
    "  url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "  return url.findall(text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'This is a website.',text)\n",
    "\n",
    "def remove_html(text):\n",
    "  html=re.compile(r'<.*?>')\n",
    "  return html.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    r = clean_tweet(input_data)\n",
    "    r = remove_URL(r)\n",
    "    r = remove_html(r)\n",
    "    r = remove_emoji(r)\n",
    "    r = str.lower(r)\n",
    "    return r"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "all_tweets = np.append(X_train, X_test)\n",
    "def normalization(tweets):\n",
    "    result = []\n",
    "    for tweet in tweets:\n",
    "        result.append(custom_standardization(tweet))\n",
    "    return result\n",
    "\n",
    "all_tweets = normalization(all_tweets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Model constants.\n",
    "max_features = 20000\n",
    "embedding_dim = 128\n",
    "max_len = 500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's make a text-only dataset (no labels):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Let's call `adapt`:\n",
    "vectorize_layer.adapt(all_tweets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apply it to the text dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from keras.metrics import AUC\n",
    "def build_model():\n",
    "\n",
    "    text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
    "    x = vectorize_layer(text_input)\n",
    "    x = Embedding(max_features + 1, embedding_dim)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x1 = Bidirectional(LSTM(units=512, return_sequences=True))(x)\n",
    "    x2 = Bidirectional(LSTM(units=256, return_sequences=True))(x1)\n",
    "\n",
    "    z2 = Bidirectional(GRU(units=512, return_sequences=True))(x)\n",
    "    z3 = Bidirectional(GRU(units=256, return_sequences=True))(z2)\n",
    "    c = Concatenate(axis=2)([x2, z3])\n",
    "\n",
    "    x3 = Bidirectional(LSTM(units=128, return_sequences=True))(c)\n",
    "    # Conv1D + global max pooling\n",
    "\n",
    "    x = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "    model = tf.keras.Model(text_input, predictions)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[AUC(name = 'auc')])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization (TextVector  (None, 500)         0           ['text[0][0]']                   \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 500, 128)     2560128     ['text_vectorization[0][0]']     \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 500, 128)     0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 500, 1024)    2625536     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 500, 1024)   1972224     ['dropout[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 500, 512)    2623488     ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  (None, 500, 512)    1969152     ['bidirectional_2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 500, 1024)    0           ['bidirectional_1[0][0]',        \n",
      "                                                                  'bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirectional  (None, 500, 256)    1180672     ['concatenate[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 256)         0           ['bidirectional_4[0][0]']        \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          32896       ['global_max_pooling1d[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " predictions (Dense)            (None, 1)            129         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,964,225\n",
      "Trainable params: 12,964,225\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# from keras.callbacks import *\n",
    "#\n",
    "# lr = ReduceLROnPlateau(monitor=\"val_auc\", mode='max', factor=0.7, patience=4, verbose=False)\n",
    "# es = EarlyStopping(monitor='val_auc',mode='max', patience=10, verbose=False,restore_best_weights=True)\n",
    "#\n",
    "# history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=60, batch_size=128,\n",
    "#                     callbacks=[es,lr])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def plot_hist(hist, metric='auc', ax=None, fold=0):\n",
    "    if ax==None:\n",
    "        plt.plot(hist.history[metric])\n",
    "        plt.plot(hist.history[\"val_\" + metric])\n",
    "        plt.title(f\"model performance\")\n",
    "        plt.ylabel(\"area_under_curve\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "        plt.show()\n",
    "        return\n",
    "    else:\n",
    "        ax.plot(hist.history[metric])\n",
    "        ax.plot(hist.history[\"val_\" + metric])\n",
    "        ax.set_title(f\"model performance fold {fold}\")\n",
    "        ax.set_ylabel(\"area_under_curve\")\n",
    "        ax.set_xlabel(\"epoch\")\n",
    "        ax.legend([\"train\", \"validation\"], loc=\"upper left\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import *\n",
    "import gc\n",
    "\n",
    "\n",
    "def fit_model(nfold, epochs=60, batch_size=32, verbose=False):\n",
    "    test_preds = []\n",
    "    auc = []\n",
    "    ncols = 5 if nfold > 5 else nfold\n",
    "    nrows = int(round(nfold / ncols))\n",
    "\n",
    "    col, row = 0, 0\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(16, round(nrows*16/ncols)))\n",
    "\n",
    "    kf = KFold(n_splits=nfold)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "        print(f\"Fold: {fold+1}\", end=' ')\n",
    "        X_train_part, X_valid = X_train[train_idx], X_train[test_idx]\n",
    "        y_train_part, y_valid = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "        lr = ReduceLROnPlateau(monitor=\"val_auc\", mode='max', factor=0.7, patience=4, verbose=False)\n",
    "        es = EarlyStopping(monitor='val_auc',mode='max', patience=10, verbose=False,restore_best_weights=True)\n",
    "        model = build_model()\n",
    "        history = model.fit(X_train_part, y_train_part, validation_data=(X_valid, y_valid), epochs=epochs, batch_size=batch_size,\n",
    "                            callbacks=[es,lr], verbose=verbose)\n",
    "\n",
    "        y_pred = model.predict(X_valid).squeeze()\n",
    "        auc_score = roc_auc_score(y_valid, y_pred)\n",
    "        print(f'auc: {round(auc_score, 5)}')\n",
    "        test_preds.append(model.predict(X_test).squeeze())\n",
    "        auc.append(auc_score)\n",
    "\n",
    "        plot_hist(history, metric='auc', ax=axes[col] if nrows <= 1 else axes[row][col], fold=fold+1)\n",
    "        del X_train_part, X_valid, y_train_part, y_valid, history, model\n",
    "        gc.collect()\n",
    "\n",
    "        col += 1\n",
    "        if col >= ncols:\n",
    "            row += 1\n",
    "            col = 0\n",
    "    return test_preds, auc\n",
    "\n",
    "folds = 4\n",
    "(test_preds, auc) = fit_model(folds, epochs=15, batch_size=128)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}