{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\boshi\\GitHub\\kears_learning\\training_project\\Natural Language Processing with Disaster Tweets\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": "   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.columns)\n",
    "train.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_label = train.target.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "y_train = train_label.reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train.drop(['id', 'keyword', 'location', 'target'], axis=1, inplace=True)\n",
    "test.drop(['id', 'keyword', 'location'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train = train.to_numpy()\n",
    "test = test.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "X_train = train.reshape(-1)\n",
    "X_test = test.reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "max_features = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "con = np.append(X_train, X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n       'Forest fire near La Ronge Sask. Canada',\n       \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\",\n       ..., 'Green Line derailment in Chicago http://t.co/UtbXLcBIuY',\n       'MEG issues Hazardous Weather Outlook (HWO) http://t.co/3X6RBQJHn3',\n       '#CityofCalgary has activated its Municipal Emergency Plan. #yycstorm'],\n      dtype=object)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29319 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "# We create a tokenizer, configured to only take\n",
    "# into account the top-1000 most common words\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "# This builds the word index\n",
    "tokenizer.fit_on_texts(con)\n",
    "\n",
    "# This turns strings into lists of integer indices.\n",
    "sequences = tokenizer.texts_to_sequences(con)\n",
    "\n",
    "# This is how you can recover the word index that was computed\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10876\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "X_train = sequences[:len(X_train)]\n",
    "X_test = sequences[len(X_train):]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from keras import layers, Model\n",
    "from keras.metrics import AUC\n",
    "\n",
    "def build_model():\n",
    "    # model = Sequential()\n",
    "    # model.add(layers.Embedding(input_length=max_features, input_dim=maxlen, output_dim=128))\n",
    "    # model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
    "    # model.add(layers.Bidirectional(layers.LSTM(64)))\n",
    "    # model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    # model.compile(optimizer='adam',\n",
    "    #                   loss='binary_crossentropy',\n",
    "    #                   metrics=[AUC(name = 'auc')])\n",
    "    # Input for variable-length sequences of integers\n",
    "    inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
    "    # Embed each integer in a 128-dimensional vector\n",
    "    x = layers.Embedding(max_features, 128)(inputs)\n",
    "    # Add 2 bidirectional LSTMs\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    # Add a classifier\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=[AUC(name = 'auc')])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 128)         2560000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 128)        98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,757,761\n",
      "Trainable params: 2,757,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_summary = build_model()\n",
    "model_summary.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_27136/3587325825.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[0mfolds\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m \u001B[1;33m(\u001B[0m\u001B[0mtest_preds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mauc\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfit_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfolds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m60\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m64\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_27136/3587325825.py\u001B[0m in \u001B[0;36mfit_model\u001B[1;34m(nfold, epochs, batch_size, verbose)\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mfold\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtrain_idx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_idx\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkfold\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Fold: {fold+1}\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m' '\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m         \u001B[0mX_train_part\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_valid\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mX_train\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtrain_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_train\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtest_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m         \u001B[0my_train_part\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_valid\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtrain_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtest_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "def fit_model(nfold, epochs=60, batch_size=32, verbose=False):\n",
    "    test_preds = []\n",
    "    auc = []\n",
    "    kfold = KFold(n_splits=nfold, shuffle=True)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(X_train, y_train)):\n",
    "        print(f\"Fold: {fold+1}\", end=' ')\n",
    "        X_train_part, X_valid = X_train[train_idx], X_train[test_idx]\n",
    "        y_train_part, y_valid = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "\n",
    "        lr = ReduceLROnPlateau(monitor=\"val_auc\", mode='max', factor=0.7, patience=4, verbose=False)\n",
    "        es = EarlyStopping(monitor='val_auc',mode='max', patience=10, verbose=False,restore_best_weights=True)\n",
    "        model = build_model()\n",
    "        history = model.fit(X_train_part, y_train_part, validation_data=(X_valid, y_valid), epochs=epochs, batch_size=batch_size,\n",
    "                            callbacks=[es,lr], verbose=verbose)\n",
    "\n",
    "        y_pred = model.predict(X_valid).squeeze()\n",
    "        auc_score = roc_auc_score(y_valid, y_pred)\n",
    "        print(f'auc: {round(auc_score, 5)}')\n",
    "        test_preds.append(model.predict(X_test).squeeze())\n",
    "        auc.append(auc_score)\n",
    "        del X_train_part, X_valid, y_train_part, y_valid, history, model\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    return test_preds, auc\n",
    "\n",
    "folds = 4\n",
    "(test_preds, auc) = fit_model(folds, epochs=60, batch_size=64, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}